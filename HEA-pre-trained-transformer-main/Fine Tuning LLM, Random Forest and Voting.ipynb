{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f96c2eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset_statistics.txt'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################### Dataset Analysis ############################\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Final_Formated_and_cleaned_file_With_Features.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to get element count in a composition\n",
    "def count_elements(composition):\n",
    "    return len(re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition))\n",
    "\n",
    "# Get unique phases and their counts\n",
    "phase_counts = df['Phase'].value_counts()\n",
    "\n",
    "# Get the percentage of each phase\n",
    "total_instances = len(df)\n",
    "phase_percentage = (phase_counts / total_instances) * 100\n",
    "\n",
    "# Get the phase with the maximum instances\n",
    "max_phase = phase_counts.idxmax()\n",
    "max_phase_count = phase_counts.max()\n",
    "\n",
    "# Get the total number of features (excluding 'composition' and 'Phase' columns)\n",
    "total_features = df.shape[1] - 2\n",
    "\n",
    "# Get the number of compositions containing N elements\n",
    "composition_counts = df['composition'].apply(count_elements).value_counts().sort_index()\n",
    "\n",
    "phase_counts, phase_percentage, total_instances, max_phase, max_phase_count, total_features, composition_counts\n",
    "\n",
    "\n",
    "# Write the statistics to a text file\n",
    "statistics_text = \"\"\"\n",
    "### Dataset Statistics\n",
    "\n",
    "#### General Information\n",
    "- **Total Instances**: 1184\n",
    "- **Total Features**: 14\n",
    "\n",
    "#### Phase Statistics\n",
    "- **Unique Phases**: 8\n",
    "  - **BCC**: 386 instances (32.60%)\n",
    "  - **FCC**: 352 instances (29.73%)\n",
    "  - **BCC+Sec**: 123 instances (10.39%)\n",
    "  - **FCC+Sec**: 99 instances (8.36%)\n",
    "  - **FCC+BCC**: 71 instances (6.00%)\n",
    "  - **FCC+BCC+Sec**: 58 instances (4.90%)\n",
    "  - **HCP**: 54 instances (4.56%)\n",
    "  - **Sec**: 41 instances (3.46%)\n",
    "\n",
    "- **Phase with Maximum Instances**: BCC (386 instances)\n",
    "\n",
    "#### Composition Statistics\n",
    "- **Compositions Containing N Elements**:\n",
    "  - **2 Elements**: 425 compositions\n",
    "  - **3 Elements**: 70 compositions\n",
    "  - **4 Elements**: 86 compositions\n",
    "  - **5 Elements**: 315 compositions\n",
    "  - **6 Elements**: 221 compositions\n",
    "  - **7 Elements**: 59 compositions\n",
    "  - **8 Elements**: 4 compositions\n",
    "  - **9 Elements**: 4 compositions\n",
    "\"\"\"\n",
    "\n",
    "# Save to a text file\n",
    "statistics_file_path = 'dataset_statistics.txt'\n",
    "with open(statistics_file_path, 'w') as f:\n",
    "    f.write(statistics_text)\n",
    "\n",
    "statistics_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ded321a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4976), started 0:07:52 ago. (Use '!kill 4976' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-99bfc404edd1e14c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-99bfc404edd1e14c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard  (optional)\n",
    "#sto comand prompt\n",
    "#taskkill /im tensorboard.exe /f\n",
    "#del /q %TMP%\\.tensorboard-info\\*\n",
    "%load_ext tensorboard\n",
    "#%tensorboard --logdir runs/train\n",
    "%tensorboard --logdir ./logs\n",
    "#%tensorboard --logdir {logs_base_dir}  --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cf35a93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': './results/pretrained_BERT_200k'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 89>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m data_train, data_test \u001b[38;5;241m=\u001b[39m train_test_split(data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# # Initialize tokenizer and model\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results/pretrained_BERT_200k\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/pretrained_BERT_200k\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(label_encoder\u001b[38;5;241m.\u001b[39mclasses_))\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Tokenize\u001b[39;00m\n",
      "File \u001b[1;32mH:\\Dev\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:642\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    641\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    644\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mH:\\Dev\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:486\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m    485\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 486\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    502\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mH:\\Dev\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32mH:\\Dev\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:112\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[0;32m    110\u001b[0m ):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 112\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mH:\\Dev\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './results/pretrained_BERT_200k'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "################################################  Esembled Model All Layers Fine Tune  #############################\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import TrainerCallback, IntervalStrategy\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import re\n",
    "import os\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import re\n",
    "\n",
    "def custom_tokenize(composition):\n",
    "    matches = re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition)\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[0])\n",
    "    return ' '.join([f\"{element}{fraction}\" for element, fraction in sorted_matches])\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Final_Formated_and_cleaned_file_No_Features.csv')\n",
    "\n",
    "# Tokenize and normalize\n",
    "data['tokenized_elements'] = data['composition'].apply(custom_tokenize)\n",
    "label_encoder = LabelEncoder()\n",
    "data['encoded_phase'] = label_encoder.fit_transform(data['Phase'])\n",
    "\n",
    "feature_columns = [col for col in data.columns if col not in ['composition', 'Phase', 'tokenized_elements', 'encoded_phase']]\n",
    "for feature in feature_columns:\n",
    "    scaler = StandardScaler()\n",
    "    data[f'normalized_{feature}'] = scaler.fit_transform(data[[feature]])\n",
    "\n",
    "data['combined_features'] = data['tokenized_elements'] + ' ' + data[[f'normalized_{feature}' for feature in feature_columns]].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Split data\n",
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./results/pretrained_BERT_200k')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./results/pretrained_BERT_200k', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Tokenize\n",
    "train_encodings = tokenizer(data_train['combined_features'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(data_test['combined_features'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, data_train['encoded_phase'].values)\n",
    "test_dataset = CustomDataset(test_encodings, data_test['encoded_phase'].values)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "# optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_dataset) * 30\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = './logs/' + current_time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='d:',\n",
    "    num_train_epochs=9,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=1,\n",
    "    save_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, lr_scheduler)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "class_report_str = classification_report(predictions.label_ids, pred_labels, target_names=label_encoder.classes_)\n",
    "print(\"Classification Report:\\n\", class_report_str)\n",
    "\n",
    "with open('./results/classification_report.txt', 'w') as f:\n",
    "    f.write(class_report_str)\n",
    "\n",
    "conf_mat = confusion_matrix(predictions.label_ids, pred_labels)\n",
    "print(\"Confusion Matrix:\", conf_mat)\n",
    "\n",
    "with open('./results/confusion_matrix.txt', 'w') as f:\n",
    "    np.savetxt(f, conf_mat, fmt='%d')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('./results/confusion_matrix.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "tokenizer.save_pretrained('./results')\n",
    "model.save_pretrained('./results')\n",
    "\n",
    "\n",
    "#################################### Random Forest #######################################\n",
    "# Load the data\n",
    "df = pd.read_csv('Final_Formated_and_cleaned_file_With_Features.csv')\n",
    "# df = pd.read_csv('Fine Tuning Medium and HEA_Rounded_Cleaned_Acoustics.csv')\n",
    "\n",
    "def get_element_fraction(composition, element):\n",
    "    if element in composition:\n",
    "        # Extract the portion of the string after the element's name\n",
    "        remainder = composition[composition.index(element) + len(element):]\n",
    "        \n",
    "        # Extract the coefficient using regex\n",
    "        import re\n",
    "        match = re.search(r\"(\\d+(\\.\\d+)?)\", remainder)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    return 0.0\n",
    "\n",
    "# Create columns for each metallic element and fill with its coefficient\n",
    "METALLIC_ELEMENTS = [\"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\",\n",
    "    \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"Se\",\n",
    "    \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\", \"Te\",\n",
    "    \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\",\n",
    "    \"Yb\", \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Tl\", \"Pb\", \"Po\", \"Th\", \"Pa\", \"U\"]\n",
    "\n",
    "for element in METALLIC_ELEMENTS:\n",
    "    df[element] = df['composition'].apply(lambda x: get_element_fraction(x, element))\n",
    "\n",
    "\n",
    "# Drop the original 'composition' column and 'hardness' column\n",
    "X = df.drop(columns=['composition', 'Phase'])\n",
    "y = df['Phase']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=60, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Save the Random Forest model\n",
    "joblib.dump(clf, './results/random_forest_model.pkl')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# BERT Predictions\n",
    "bert_predictions = trainer.predict(test_dataset)\n",
    "bert_pred_labels = np.argmax(bert_predictions.predictions, axis=1)\n",
    "bert_pred_probs = scipy.special.softmax(bert_predictions.predictions, axis=1)  # Converting logits to probabilities\n",
    "\n",
    "# Transform BERT labels back to original (string) labels\n",
    "bert_pred_labels_str = label_encoder.inverse_transform(bert_pred_labels)\n",
    "\n",
    "# Random Forest Predictions\n",
    "rf_pred_labels = clf.predict(X_test)\n",
    "rf_pred_probs = clf.predict_proba(X_test)\n",
    "\n",
    "# Make sure that both y_test and final_predictions are of the same type (either both numbers or both strings)\n",
    "# Here, converting everything to string type\n",
    "y_test_str = y_test.astype(str)\n",
    "rf_pred_labels_str = rf_pred_labels.astype(str)\n",
    "\n",
    "# Weighted Voting\n",
    "bert_weight = 0.3\n",
    "rf_weight = 0.7\n",
    "final_predictions_weighted = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    avg_prob = b_prob * bert_weight + r_prob * rf_weight\n",
    "    final_prediction = np.argmax(avg_prob)\n",
    "    final_predictions_weighted.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Soft Voting with Probabilities\n",
    "final_predictions_soft = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    avg_prob = (b_prob + r_prob) / 2\n",
    "    final_prediction = np.argmax(avg_prob)\n",
    "    final_predictions_soft.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Confidence-based Voting\n",
    "final_predictions_confidence = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    b_confidence = np.max(b_prob)\n",
    "    r_confidence = np.max(r_prob)\n",
    "    final_prediction = np.argmax(b_prob) if b_confidence > r_confidence else np.argmax(r_prob)\n",
    "    final_predictions_confidence.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Assume final_predictions holds the ensemble predictions you want to evaluate\n",
    "final_predictions = final_predictions_confidence  # or final_predictions_soft or final_predictions_confidence\n",
    "\n",
    "# Generate the confusion matrix\n",
    "final_cm = confusion_matrix(y_test_str, final_predictions)\n",
    "\n",
    "# Calculate accuracy from the confusion matrix\n",
    "final_accuracy = np.trace(final_cm) / np.sum(final_cm)\n",
    "\n",
    "# Print the calculated accuracy\n",
    "print(\"Final Model Accuracy:\", final_accuracy)\n",
    "\n",
    "# Debug Step 4: Check Confusion Matrix Labels\n",
    "sorted_labels = sorted(y_test.unique())\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(final_cm, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=sorted_labels, yticklabels=sorted_labels)\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Ensemble Model Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the different ensemble methods\n",
    "print(\"Weighted Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_weighted))\n",
    "print(\"Soft Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_soft))\n",
    "print(\"Confidence-based Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_confidence))\n",
    "\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nEnsemble Model Classification Report:\\n\", classification_report(y_test, final_predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd63259",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Frozen Layers Training ############################\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import TrainerCallback, IntervalStrategy\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import re\n",
    "import os\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import re\n",
    "\n",
    "def custom_tokenize(composition):\n",
    "    matches = re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition)\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[0])\n",
    "    return ' '.join([f\"{element}{fraction}\" for element, fraction in sorted_matches])\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Final_Formated_and_cleaned_file_With_Features_Rounded_Cleaned_Acoustics.csv')\n",
    "\n",
    "# Tokenize and normalize\n",
    "data['tokenized_elements'] = data['composition'].apply(custom_tokenize)\n",
    "label_encoder = LabelEncoder()\n",
    "data['encoded_phase'] = label_encoder.fit_transform(data['Phase'])\n",
    "\n",
    "feature_columns = [col for col in data.columns if col not in ['composition', 'Phase', 'tokenized_elements', 'encoded_phase']]\n",
    "for feature in feature_columns:\n",
    "    scaler = StandardScaler()\n",
    "    data[f'normalized_{feature}'] = scaler.fit_transform(data[[feature]])\n",
    "\n",
    "data['combined_features'] = data['tokenized_elements'] + ' ' + data[[f'normalized_{feature}' for feature in feature_columns]].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Split data\n",
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./results/pretrained_BERT_200K')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./results/pretrained_BERT_200K', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "train_encodings = tokenizer(data_train['combined_features'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(data_test['combined_features'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, data_train['encoded_phase'].values)\n",
    "test_dataset = CustomDataset(test_encodings, data_test['encoded_phase'].values)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "# Initialize custom optimizer with weight decay for specific layers\n",
    "decay_layers = [\"1\", \"5\", \"9\", \"12\"]\n",
    "decay_param_names = [n for n, p in model.named_parameters() if any(f\".{layer}.\" in n for layer in decay_layers)]\n",
    "no_decay_param_names = [n for n, p in model.named_parameters() if n not in decay_param_names]\n",
    "decay_params = [p for n, p in model.named_parameters() if n in decay_param_names]\n",
    "no_decay_params = [p for n, p in model.named_parameters() if n in no_decay_param_names]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": decay_params, \"weight_decay\": 0.04},\n",
    "    {\"params\": no_decay_params, \"weight_decay\": 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "\n",
    "# Scheduler\n",
    "num_training_steps = len(train_dataset) * 30\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = './logs/' + current_time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='d:',\n",
    "    num_train_epochs=13,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=1,\n",
    "    save_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, lr_scheduler)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "class_report_str = classification_report(predictions.label_ids, pred_labels, target_names=label_encoder.classes_)\n",
    "print(\"Classification Report:\\n\", class_report_str)\n",
    "\n",
    "with open('./results/classification_report.txt', 'w') as f:\n",
    "    f.write(class_report_str)\n",
    "\n",
    "conf_mat = confusion_matrix(predictions.label_ids, pred_labels)\n",
    "print(\"Confusion Matrix:\", conf_mat)\n",
    "\n",
    "with open('./results/confusion_matrix.txt', 'w') as f:\n",
    "    np.savetxt(f, conf_mat, fmt='%d')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('./results/confusion_matrix.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "tokenizer.save_pretrained('d:')\n",
    "model.save_pretrained('d:')\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('Final_Formated_and_cleaned_file_With_Features_Rounded_Cleaned_Acoustics.csv')\n",
    "# df = pd.read_csv('Fine Tuning Medium and HEA_Rounded_Cleaned.csv')\n",
    "\n",
    "def get_element_fraction(composition, element):\n",
    "    if element in composition:\n",
    "        # Extract the portion of the string after the element's name\n",
    "        remainder = composition[composition.index(element) + len(element):]\n",
    "        \n",
    "        # Extract the coefficient using regex\n",
    "        import re\n",
    "        match = re.search(r\"(\\d+(\\.\\d+)?)\", remainder)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    return 0.0\n",
    "\n",
    "# Create columns for each metallic element and fill with its coefficient\n",
    "METALLIC_ELEMENTS = [\"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\",\n",
    "    \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"Se\",\n",
    "    \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\", \"Te\",\n",
    "    \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\",\n",
    "    \"Yb\", \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Tl\", \"Pb\", \"Po\", \"Th\", \"Pa\", \"U\"]\n",
    "\n",
    "for element in METALLIC_ELEMENTS:\n",
    "    df[element] = df['composition'].apply(lambda x: get_element_fraction(x, element))\n",
    "\n",
    "\n",
    "# Drop the original 'composition' column and 'hardness' column\n",
    "X = df.drop(columns=['composition', 'Phase'])\n",
    "y = df['Phase']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Save the Random Forest model\n",
    "joblib.dump(clf, './results/random_forest_model.pkl')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# BERT Predictions\n",
    "bert_predictions = trainer.predict(test_dataset)\n",
    "bert_pred_labels = np.argmax(bert_predictions.predictions, axis=1)\n",
    "bert_pred_probs = scipy.special.softmax(bert_predictions.predictions, axis=1)  # Converting logits to probabilities\n",
    "\n",
    "# Transform BERT labels back to original (string) labels\n",
    "bert_pred_labels_str = label_encoder.inverse_transform(bert_pred_labels)\n",
    "\n",
    "# Random Forest Predictions\n",
    "rf_pred_labels = clf.predict(X_test)\n",
    "rf_pred_probs = clf.predict_proba(X_test)\n",
    "\n",
    "# Make sure that both y_test and final_predictions are of the same type (either both numbers or both strings)\n",
    "# Here, converting everything to string type\n",
    "y_test_str = y_test.astype(str)\n",
    "rf_pred_labels_str = rf_pred_labels.astype(str)\n",
    "\n",
    "# Weighted Voting\n",
    "bert_weight = 0.3\n",
    "rf_weight = 0.7\n",
    "final_predictions_weighted = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    avg_prob = b_prob * bert_weight + r_prob * rf_weight\n",
    "    final_prediction = np.argmax(avg_prob)\n",
    "    final_predictions_weighted.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Soft Voting with Probabilities\n",
    "final_predictions_soft = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    avg_prob = (b_prob + r_prob) / 2\n",
    "    final_prediction = np.argmax(avg_prob)\n",
    "    final_predictions_soft.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Confidence-based Voting\n",
    "final_predictions_confidence = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    b_confidence = np.max(b_prob)\n",
    "    r_confidence = np.max(r_prob)\n",
    "    final_prediction = np.argmax(b_prob) if b_confidence > r_confidence else np.argmax(r_prob)\n",
    "    final_predictions_confidence.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Assume final_predictions holds the ensemble predictions you want to evaluate\n",
    "final_predictions = final_predictions_weighted  # or final_predictions_soft or final_predictions_confidence\n",
    "\n",
    "# Generate the confusion matrix\n",
    "final_cm = confusion_matrix(y_test_str, final_predictions)\n",
    "\n",
    "# Calculate accuracy from the confusion matrix\n",
    "final_accuracy = np.trace(final_cm) / np.sum(final_cm)\n",
    "\n",
    "# Print the calculated accuracy\n",
    "print(\"Final Model Accuracy:\", final_accuracy)\n",
    "\n",
    "\n",
    "# Debug Step 4: Check Confusion Matrix Labels\n",
    "sorted_labels = sorted(y_test.unique())\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(final_cm, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=sorted_labels, yticklabels=sorted_labels)\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Ensemble Model Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the different ensemble methods\n",
    "print(\"Weighted Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_weighted))\n",
    "print(\"Soft Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_soft))\n",
    "print(\"Confidence-based Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_confidence))\n",
    "\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nEnsemble Model Classification Report:\\n\", classification_report(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cdfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################  Heat Maps ############################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "\n",
    "# Function to tokenize and sort elements\n",
    "def custom_tokenize(composition):\n",
    "    matches = re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition)\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[0])\n",
    "    return ' '.join([''.join(pair) for pair in sorted_matches])\n",
    "\n",
    "# Load your trained model and tokenizer\n",
    "model_path = './results'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, output_attentions=True)\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Original composition\n",
    "composition = \"Co1 Cr1 Fe1 Mn1 Ni1 V1\"\n",
    "tokenized_composition = custom_tokenize(composition)\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(tokenized_composition, return_tensors=\"pt\", add_special_tokens=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Get Attention Weights\n",
    "outputs = model(**inputs)\n",
    "attentions = outputs.attentions  # List of attention weights for each layer\n",
    "\n",
    "# Aggregate across all heads for simplicity (could be refined)\n",
    "avg_attention = attentions[-1].squeeze(0).mean(0).detach().numpy()\n",
    "\n",
    "# Exclude the [CLS] and [SEP] tokens\n",
    "avg_attention = avg_attention[1:-1, 1:-1]\n",
    "\n",
    "# Average the attention values for element pairs\n",
    "element_count = len(re.findall(r'([A-Z][a-z]*[0-9.]+)', tokenized_composition))\n",
    "avg_attention_relevant = np.zeros((element_count, element_count))\n",
    "\n",
    "for i in range(0, avg_attention.shape[0], 2):\n",
    "    for j in range(0, avg_attention.shape[1], 2):\n",
    "        avg_value = np.mean(avg_attention[i:i+2, j:j+2])\n",
    "        avg_attention_relevant[i//2, j//2] = avg_value\n",
    "\n",
    "# Symmetrize the matrix\n",
    "avg_attention_symmetric = (avg_attention_relevant + avg_attention_relevant.T) / 2\n",
    "\n",
    "# Visualize\n",
    "labels = custom_tokenize(composition).split()\n",
    "sns.heatmap(avg_attention_symmetric, annot=True, xticklabels=labels, yticklabels=labels)\n",
    "plt.show()\n",
    "\n",
    "# Visualize\n",
    "labels = custom_tokenize(composition).split()\n",
    "sns.heatmap(avg_attention_symmetric, annot=True, xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"heatmap_300dpi.png\", dpi=300)  # Saving with 300dpi resolution\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697754e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc567ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136dc684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8884460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Final_Formated_and_cleaned_file_No_Features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "825ab917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composition</th>\n",
       "      <th>Phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al1 Nb1 Ta1 Ti1</td>\n",
       "      <td>BCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hf1 Mo0.5 Nb1 Ti1 V0.5</td>\n",
       "      <td>BCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hf1 Mo1 Nb1 Ta1 Ti1 Zr1</td>\n",
       "      <td>BCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hf1 Mo1 Nb1 Ti1 Zr1</td>\n",
       "      <td>BCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hf1 Mo1 Ta1 Ti1 Zr1</td>\n",
       "      <td>BCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>Co1 Fe1 Mn1 Ti1 V1 Zr2.3</td>\n",
       "      <td>Sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>Co1 Fe1 Mn1 Ti1 V1 Zr2.6</td>\n",
       "      <td>Sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>Co1 Fe1 Mn1 Ti1 V1 Zr3</td>\n",
       "      <td>Sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>Al1 Cu0.2 Li0.5 Mg1 Zn0.5</td>\n",
       "      <td>Sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>Al0.5 B1 Co1 Cr1 Cu1 Fe1 Ni1</td>\n",
       "      <td>Sec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1184 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       composition Phase\n",
       "0                  Al1 Nb1 Ta1 Ti1   BCC\n",
       "1           Hf1 Mo0.5 Nb1 Ti1 V0.5   BCC\n",
       "2          Hf1 Mo1 Nb1 Ta1 Ti1 Zr1   BCC\n",
       "3              Hf1 Mo1 Nb1 Ti1 Zr1   BCC\n",
       "4              Hf1 Mo1 Ta1 Ti1 Zr1   BCC\n",
       "...                            ...   ...\n",
       "1179      Co1 Fe1 Mn1 Ti1 V1 Zr2.3   Sec\n",
       "1180      Co1 Fe1 Mn1 Ti1 V1 Zr2.6   Sec\n",
       "1181        Co1 Fe1 Mn1 Ti1 V1 Zr3   Sec\n",
       "1182     Al1 Cu0.2 Li0.5 Mg1 Zn0.5   Sec\n",
       "1183  Al0.5 B1 Co1 Cr1 Cu1 Fe1 Ni1   Sec\n",
       "\n",
       "[1184 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98ccb108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and sort elements\n",
    "def custom_tokenize(composition):\n",
    "    matches = re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition)\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[0])\n",
    "    return ' '.join([''.join(pair) for pair in sorted_matches])\n",
    "\n",
    "# Apply the function to the data\n",
    "data['tokenized_elements'] = data['composition'].apply(custom_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fe1157d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Al1 Nb1 Ta1 Ti1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['composition'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "228f2e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Al1 Nb1 Ta1 Ti1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokenized_elements'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3606902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composition</th>\n",
       "      <th>Phase</th>\n",
       "      <th>tokenized_elements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Mn0.253 Cr0.747</td>\n",
       "      <td>BCC</td>\n",
       "      <td>Cr0.747 Mn0.253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Mn0.43 Cr0.57</td>\n",
       "      <td>BCC</td>\n",
       "      <td>Cr0.57 Mn0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Mn0.5 Cr0.5</td>\n",
       "      <td>BCC</td>\n",
       "      <td>Cr0.5 Mn0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Mn0.6 Cr0.4</td>\n",
       "      <td>BCC</td>\n",
       "      <td>Cr0.4 Mn0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Mn0.67 Cr0.33</td>\n",
       "      <td>BCC</td>\n",
       "      <td>Cr0.33 Mn0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>Ti0.96 Ta0.04</td>\n",
       "      <td>HCP</td>\n",
       "      <td>Ta0.04 Ti0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>Ti0.987 Ta0.013</td>\n",
       "      <td>HCP</td>\n",
       "      <td>Ta0.013 Ti0.987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>Ti0.97 Nb0.03</td>\n",
       "      <td>HCP</td>\n",
       "      <td>Nb0.03 Ti0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>Zr0.7 Y0.3</td>\n",
       "      <td>HCP</td>\n",
       "      <td>Y0.3 Zr0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>Zr0.953 Nb0.047</td>\n",
       "      <td>HCP</td>\n",
       "      <td>Nb0.047 Zr0.953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          composition Phase tokenized_elements\n",
       "87    Mn0.253 Cr0.747   BCC    Cr0.747 Mn0.253\n",
       "88      Mn0.43 Cr0.57   BCC      Cr0.57 Mn0.43\n",
       "89        Mn0.5 Cr0.5   BCC        Cr0.5 Mn0.5\n",
       "90        Mn0.6 Cr0.4   BCC        Cr0.4 Mn0.6\n",
       "91      Mn0.67 Cr0.33   BCC      Cr0.33 Mn0.67\n",
       "...               ...   ...                ...\n",
       "1123    Ti0.96 Ta0.04   HCP      Ta0.04 Ti0.96\n",
       "1124  Ti0.987 Ta0.013   HCP    Ta0.013 Ti0.987\n",
       "1125    Ti0.97 Nb0.03   HCP      Nb0.03 Ti0.97\n",
       "1130       Zr0.7 Y0.3   HCP         Y0.3 Zr0.7\n",
       "1136  Zr0.953 Nb0.047   HCP    Nb0.047 Zr0.953\n",
       "\n",
       "[197 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['composition']!=data['tokenized_elements']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418558d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9af22f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composition</th>\n",
       "      <th>Phase</th>\n",
       "      <th>tokenized_elements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al1 Nb1 Ta1 Ti1</td>\n",
       "      <td>BCC</td>\n",
       "      <td>Al1 Nb1 Ta1 Ti1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hf1 Mo0.5 Nb1 Ti1 V0.5</td>\n",
       "      <td>BCC</td>\n",
       "      <td>Hf1 Mo0.5 Nb1 Ti1 V0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hf1 Mo1 Nb1 Ta1 Ti1 Zr1</td>\n",
       "      <td>BCC</td>\n",
       "      <td>Hf1 Mo1 Nb1 Ta1 Ti1 Zr1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hf1 Mo1 Nb1 Ti1 Zr1</td>\n",
       "      <td>BCC</td>\n",
       "      <td>Hf1 Mo1 Nb1 Ti1 Zr1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hf1 Mo1 Ta1 Ti1 Zr1</td>\n",
       "      <td>BCC</td>\n",
       "      <td>Hf1 Mo1 Ta1 Ti1 Zr1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>Co1 Fe1 Mn1 Ti1 V1 Zr2.3</td>\n",
       "      <td>Sec</td>\n",
       "      <td>Co1 Fe1 Mn1 Ti1 V1 Zr2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>Co1 Fe1 Mn1 Ti1 V1 Zr2.6</td>\n",
       "      <td>Sec</td>\n",
       "      <td>Co1 Fe1 Mn1 Ti1 V1 Zr2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>Co1 Fe1 Mn1 Ti1 V1 Zr3</td>\n",
       "      <td>Sec</td>\n",
       "      <td>Co1 Fe1 Mn1 Ti1 V1 Zr3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>Al1 Cu0.2 Li0.5 Mg1 Zn0.5</td>\n",
       "      <td>Sec</td>\n",
       "      <td>Al1 Cu0.2 Li0.5 Mg1 Zn0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>Al0.5 B1 Co1 Cr1 Cu1 Fe1 Ni1</td>\n",
       "      <td>Sec</td>\n",
       "      <td>Al0.5 B1 Co1 Cr1 Cu1 Fe1 Ni1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1184 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       composition Phase            tokenized_elements\n",
       "0                  Al1 Nb1 Ta1 Ti1   BCC               Al1 Nb1 Ta1 Ti1\n",
       "1           Hf1 Mo0.5 Nb1 Ti1 V0.5   BCC        Hf1 Mo0.5 Nb1 Ti1 V0.5\n",
       "2          Hf1 Mo1 Nb1 Ta1 Ti1 Zr1   BCC       Hf1 Mo1 Nb1 Ta1 Ti1 Zr1\n",
       "3              Hf1 Mo1 Nb1 Ti1 Zr1   BCC           Hf1 Mo1 Nb1 Ti1 Zr1\n",
       "4              Hf1 Mo1 Ta1 Ti1 Zr1   BCC           Hf1 Mo1 Ta1 Ti1 Zr1\n",
       "...                            ...   ...                           ...\n",
       "1179      Co1 Fe1 Mn1 Ti1 V1 Zr2.3   Sec      Co1 Fe1 Mn1 Ti1 V1 Zr2.3\n",
       "1180      Co1 Fe1 Mn1 Ti1 V1 Zr2.6   Sec      Co1 Fe1 Mn1 Ti1 V1 Zr2.6\n",
       "1181        Co1 Fe1 Mn1 Ti1 V1 Zr3   Sec        Co1 Fe1 Mn1 Ti1 V1 Zr3\n",
       "1182     Al1 Cu0.2 Li0.5 Mg1 Zn0.5   Sec     Al1 Cu0.2 Li0.5 Mg1 Zn0.5\n",
       "1183  Al0.5 B1 Co1 Cr1 Cu1 Fe1 Ni1   Sec  Al0.5 B1 Co1 Cr1 Cu1 Fe1 Ni1\n",
       "\n",
       "[1184 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114c10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9003fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e70eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Label encode the 'Phase' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['encoded_phase'] = label_encoder.fit_transform(data['Phase'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "model_path = './results'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load trained Random Forest model\n",
    "clf = joblib.load('./results/random_forest_model.pkl')\n",
    "\n",
    "# Function to predict probabilities using the ensemble model\n",
    "def predictor(texts):\n",
    "    # Prepare data for BERT\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "    bert_probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "    # Prepare data for Random Forest\n",
    "    METALLIC_ELEMENTS = [\"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\",\n",
    "    \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"Se\",\n",
    "    \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\", \"Te\",\n",
    "    \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\",\n",
    "    \"Yb\", \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Tl\", \"Pb\", \"Po\", \"Th\", \"Pa\", \"U\"]\n",
    "    \n",
    "    def get_element_fraction(composition, element):\n",
    "        if element in composition:\n",
    "            remainder = composition[composition.index(element):]\n",
    "            match = re.search(r\"(\\d+(\\.\\d+)?)\", remainder)\n",
    "            if match:\n",
    "                return float(match.group(1))\n",
    "        return 0.0\n",
    "\n",
    "    X_rf = np.array([[get_element_fraction(text, el) for el in METALLIC_ELEMENTS] for text in texts])\n",
    "    \n",
    "    # Get Random Forest probabilities\n",
    "    rf_probs = clf.predict_proba(X_rf)\n",
    "\n",
    "    # Ensemble probabilities\n",
    "    bert_weight = 0.3\n",
    "    rf_weight = 0.7\n",
    "    ensemble_probs = bert_probs * bert_weight + rf_probs * rf_weight\n",
    "\n",
    "    return ensemble_probs\n",
    "\n",
    "# Function to split text for LIME\n",
    "def custom_split(text):\n",
    "    return re.findall(r'([A-Z][a-z]*[0-9.]+)', text)\n",
    "\n",
    "# Example to be explained\n",
    "your_composition = \"Co1 Cr1 Fe1 Mn1 Ni0.8 V1\"\n",
    "sorted_composition = custom_tokenize(your_composition)\n",
    "sample_text = sorted_composition\n",
    "\n",
    "# Initialize LIME explainer\n",
    "explainer = LimeTextExplainer(class_names=label_encoder.classes_, split_expression=custom_split)\n",
    "\n",
    "# Explain the instance\n",
    "explanation = explainer.explain_instance(sample_text, predictor, num_features=len(label_encoder.classes_), top_labels=3)\n",
    "\n",
    "# Show and save the explanation\n",
    "explanation.show_in_notebook(text=sample_text)\n",
    "explanation.save_to_file('A_explanation_output.html')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract LIME figures for each class label of interest\n",
    "for label in explanation.top_labels:\n",
    "    fig = explanation.as_pyplot_figure(label=label)\n",
    "    plt.savefig(f\"lime_explanation_{label_encoder.classes_[label]}.png\", dpi=300)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37dda2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b016abb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlime_text\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LimeTextExplainer\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lime'"
     ]
    }
   ],
   "source": [
    "##################################### LIME   ###########################\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Final_Formated_and_cleaned_file_No_Features.csv')\n",
    "\n",
    "# Function to tokenize and sort elements\n",
    "def custom_tokenize(composition):\n",
    "    matches = re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition)\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[0])\n",
    "    return ' '.join([''.join(pair) for pair in sorted_matches])\n",
    "\n",
    "# Apply the function to the data\n",
    "data['tokenized_elements'] = data['composition'].apply(custom_tokenize)\n",
    "\n",
    "# Label encode the 'Phase' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['encoded_phase'] = label_encoder.fit_transform(data['Phase'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "model_path = './results'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load trained Random Forest model\n",
    "clf = joblib.load('./results/random_forest_model.pkl')\n",
    "\n",
    "# Function to predict probabilities using the ensemble model\n",
    "def predictor(texts):\n",
    "    # Prepare data for BERT\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "    bert_probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "    # Prepare data for Random Forest\n",
    "    METALLIC_ELEMENTS = [\"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\",\n",
    "    \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"Se\",\n",
    "    \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\", \"Te\",\n",
    "    \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\",\n",
    "    \"Yb\", \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Tl\", \"Pb\", \"Po\", \"Th\", \"Pa\", \"U\"]\n",
    "    \n",
    "    def get_element_fraction(composition, element):\n",
    "        if element in composition:\n",
    "            remainder = composition[composition.index(element):]\n",
    "            match = re.search(r\"(\\d+(\\.\\d+)?)\", remainder)\n",
    "            if match:\n",
    "                return float(match.group(1))\n",
    "        return 0.0\n",
    "\n",
    "    X_rf = np.array([[get_element_fraction(text, el) for el in METALLIC_ELEMENTS] for text in texts])\n",
    "    \n",
    "    # Get Random Forest probabilities\n",
    "    rf_probs = clf.predict_proba(X_rf)\n",
    "\n",
    "    # Ensemble probabilities\n",
    "    bert_weight = 0.3\n",
    "    rf_weight = 0.7\n",
    "    ensemble_probs = bert_probs * bert_weight + rf_probs * rf_weight\n",
    "\n",
    "    return ensemble_probs\n",
    "\n",
    "# Function to split text for LIME\n",
    "def custom_split(text):\n",
    "    return re.findall(r'([A-Z][a-z]*[0-9.]+)', text)\n",
    "\n",
    "# Example to be explained\n",
    "your_composition = \"Co1 Cr1 Fe1 Mn1 Ni0.8 V1\"\n",
    "sorted_composition = custom_tokenize(your_composition)\n",
    "sample_text = sorted_composition\n",
    "\n",
    "# Initialize LIME explainer\n",
    "explainer = LimeTextExplainer(class_names=label_encoder.classes_, split_expression=custom_split)\n",
    "\n",
    "# Explain the instance\n",
    "explanation = explainer.explain_instance(sample_text, predictor, num_features=len(label_encoder.classes_), top_labels=3)\n",
    "\n",
    "# Show and save the explanation\n",
    "explanation.show_in_notebook(text=sample_text)\n",
    "explanation.save_to_file('A_explanation_output.html')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract LIME figures for each class label of interest\n",
    "for label in explanation.top_labels:\n",
    "    fig = explanation.as_pyplot_figure(label=label)\n",
    "    plt.savefig(f\"lime_explanation_{label_encoder.classes_[label]}.png\", dpi=300)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc9ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
